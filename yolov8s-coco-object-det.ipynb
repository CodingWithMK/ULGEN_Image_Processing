{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ee6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df193ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (11): Concat()\n",
      "      (12): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (14): Concat()\n",
      "      (15): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (17): Concat()\n",
      "      (18): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (20): Concat()\n",
      "      (21): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8s.pt\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74b0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.202  Python-3.10.18 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./datasets/coco/coco.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.1, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2147008  ultralytics.nn.modules.head.Detect           [80, [128, 256, 512]]         \n",
      "Model summary: 129 layers, 11,166,560 parameters, 11,166,544 gradients, 28.8 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 190.894.9 MB/s, size: 146.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\datasets\\coco\\labels\\train2017.cache... 11829 images, 109 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 11829/11829 11.0Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.50.5 ms, read: 142.528.0 MB/s, size: 180.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\datasets\\coco\\labels\\val2017.cache... 5000 images, 48 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5000/5000 5.0Mit/s 0.0s\n",
      "Plotting labels to C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100      1.43G      1.099      1.181       1.16         35        512: 100% ━━━━━━━━━━━━ 1479/1479 6.6it/s 3:44<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.7it/s 40.5s<0.1s\n",
      "                   all       5000      36781      0.631      0.483      0.522      0.366\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100      1.48G      1.164      1.372      1.193         90        512: 100% ━━━━━━━━━━━━ 1479/1479 6.9it/s 3:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.535      0.404      0.412      0.277\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100      1.41G      1.317      1.741      1.292         98        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.4s<0.1s\n",
      "                   all       5000      36781      0.447      0.264      0.251      0.156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100      1.53G      1.459      2.103       1.39         47        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.6s<0.1s\n",
      "                   all       5000      36781      0.387      0.254      0.232      0.143\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100      1.49G      1.439      2.074      1.388        108        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.2s<0.1s\n",
      "                   all       5000      36781      0.434      0.273      0.256      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100      1.98G      1.419      2.005      1.372         81        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.9it/s 39.8s<0.1s\n",
      "                   all       5000      36781      0.458      0.276      0.268      0.169\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100      1.98G        1.4      1.949      1.358         48        512: 100% ━━━━━━━━━━━━ 1479/1479 7.3it/s 3:24<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.4s<0.1s\n",
      "                   all       5000      36781      0.443        0.3      0.294      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100      2.01G      1.391      1.898      1.347         87        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.5s<0.1s\n",
      "                   all       5000      36781      0.461      0.311        0.3      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100       1.4G      1.362      1.846      1.333         75        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.2s<0.1s\n",
      "                   all       5000      36781      0.472      0.317      0.315      0.204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100      1.42G      1.353      1.824      1.326         59        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.469      0.327      0.319      0.206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100      1.42G      1.333      1.772      1.317         98        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781       0.48      0.337      0.333      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100      1.42G      1.323      1.744      1.308         38        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 6.9it/s 45.2s<0.1s\n",
      "                   all       5000      36781      0.467       0.33      0.329      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100      1.42G      1.306      1.706        1.3         92        512: 100% ━━━━━━━━━━━━ 1479/1479 6.9it/s 3:36<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.6s<0.1s\n",
      "                   all       5000      36781      0.469       0.34       0.34      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100      1.42G      1.302      1.688      1.295        112        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.4s<0.1s\n",
      "                   all       5000      36781      0.489      0.345      0.343      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100      1.42G      1.291      1.664      1.288         69        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.2s<0.1s\n",
      "                   all       5000      36781      0.488      0.347      0.359      0.236\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100      1.45G      1.283      1.635      1.281        100        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.497      0.354      0.358      0.238\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100      1.45G      1.281      1.615      1.279         54        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.511      0.364      0.366      0.241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100      1.45G      1.263      1.584       1.27         36        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.503      0.365      0.372      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100      1.45G      1.266      1.572      1.268         92        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781       0.51      0.368      0.375      0.248\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100      1.45G      1.252      1.545       1.26         40        512: 100% ━━━━━━━━━━━━ 1479/1479 6.9it/s 3:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.2s<0.1s\n",
      "                   all       5000      36781      0.536      0.374      0.388      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100      1.45G      1.249      1.547      1.263         73        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.502      0.384      0.388       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100      1.45G      1.241      1.513      1.253        102        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.1s<0.1s\n",
      "                   all       5000      36781      0.521      0.374      0.385      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100      1.45G      1.234      1.495       1.25         79        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.522      0.382      0.393      0.264\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100      1.56G       1.23      1.483      1.243         65        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.541      0.382      0.398      0.267\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100      1.56G       1.22      1.462      1.239         62        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.545      0.374      0.401      0.268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100      1.56G       1.22      1.459      1.241        102        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.2s<0.1s\n",
      "                   all       5000      36781      0.539       0.39      0.405      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100      1.56G       1.21      1.437      1.233         43        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.4s<0.1s\n",
      "                   all       5000      36781      0.539      0.394      0.408      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100      1.56G      1.206      1.437      1.232         61        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.3s<0.1s\n",
      "                   all       5000      36781       0.54      0.398      0.409      0.276\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100      1.56G      1.196      1.415      1.227         89        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.1s<0.1s\n",
      "                   all       5000      36781      0.554      0.397      0.417      0.283\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100      1.56G      1.201      1.406      1.226         88        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.554      0.396      0.415      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100      1.56G      1.193      1.392      1.223         69        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.544      0.404      0.421      0.285\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100      1.56G      1.189      1.386      1.218        110        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.543      0.409      0.422      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100      1.56G      1.185      1.366      1.216         27        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.559      0.407      0.427       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100      1.56G       1.18      1.351      1.211         63        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.2it/s 43.6s<0.1s\n",
      "                   all       5000      36781      0.555       0.41      0.427      0.291\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100      1.56G      1.172       1.34      1.209         39        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.555       0.41      0.429      0.292\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100      1.56G      1.175      1.334      1.209         42        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.549      0.418      0.432      0.296\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100      1.56G      1.163      1.326      1.204         75        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.561      0.416      0.438      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100      1.56G      1.161      1.321      1.203         44        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.571      0.409      0.437        0.3\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100      1.56G      1.155      1.297      1.197         70        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.551      0.419      0.434      0.297\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100      1.56G      1.152      1.288      1.195         71        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.568      0.417      0.438      0.299\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100      1.56G      1.149      1.278      1.193         61        512: 100% ━━━━━━━━━━━━ 1479/1479 6.9it/s 3:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.1s<0.1s\n",
      "                   all       5000      36781       0.56      0.422      0.437      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100      1.56G      1.145      1.276      1.192         84        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.549       0.42      0.435      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100      1.56G      1.147      1.269      1.188         69        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.1it/s 43.9s<0.1s\n",
      "                   all       5000      36781      0.557       0.42      0.436        0.3\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100      1.56G      1.136      1.254      1.184         53        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781       0.56       0.42       0.44      0.303\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/100      1.56G      1.134      1.255      1.185        131        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.564      0.423      0.443      0.304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/100      1.56G      1.135      1.245      1.181         78        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.582      0.418      0.442      0.304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/100      1.56G      1.128      1.223      1.176         49        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.1s<0.1s\n",
      "                   all       5000      36781      0.583      0.422      0.447      0.306\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/100      1.56G      1.128      1.222      1.178         34        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.582      0.421      0.449      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/100      1.56G      1.121      1.214      1.174         57        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.563       0.43      0.448      0.307\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/100      1.56G      1.114      1.202      1.172         73        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.1s<0.1s\n",
      "                   all       5000      36781      0.564      0.435      0.451      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/100      1.56G      1.115      1.194       1.17         60        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781       0.56      0.434      0.452      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/100      1.56G      1.114      1.188      1.169         93        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.4s<0.1s\n",
      "                   all       5000      36781      0.566      0.436      0.454      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/100      1.56G      1.109      1.182      1.167         62        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.547      0.437      0.451      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/100      1.56G      1.102      1.168      1.163         56        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.545      0.445      0.452      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/100      1.56G        1.1      1.159      1.157         54        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.571      0.431      0.453      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/100      1.56G      1.093      1.151      1.157         60        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.569      0.438      0.453      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/100      1.56G      1.093       1.15      1.158         87        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.572      0.432      0.453      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/100      1.56G      1.087       1.14      1.155         71        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.561      0.435      0.454      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/100      1.56G      1.087      1.132      1.154         81        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.565      0.436      0.454      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     60/100      1.56G      1.082      1.128      1.151         85        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.3s<0.1s\n",
      "                   all       5000      36781      0.583      0.427      0.455      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     61/100      1.56G      1.081      1.118      1.151         50        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.573      0.434      0.456      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     62/100      1.56G      1.075      1.111      1.148         82        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.1it/s 44.2s<0.1s\n",
      "                   all       5000      36781      0.574      0.436      0.457      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     63/100      1.56G      1.075      1.101      1.142         49        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.585       0.43      0.458      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     64/100      1.56G       1.07      1.093      1.143         39        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.4s<0.1s\n",
      "                   all       5000      36781      0.581       0.43      0.457      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     65/100      1.56G       1.07      1.087      1.142         71        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.574      0.437      0.457      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     66/100      1.56G      1.062       1.08      1.137         53        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.583      0.432      0.458      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     67/100      1.56G      1.056      1.068      1.134        115        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.581      0.432      0.459      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     68/100      1.56G      1.054      1.063      1.134         66        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.4s<0.1s\n",
      "                   all       5000      36781      0.582      0.433      0.459      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     69/100      1.56G      1.054      1.061       1.13         90        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.574      0.436      0.459      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     70/100      1.56G      1.046      1.052      1.128         79        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 43.0s<0.1s\n",
      "                   all       5000      36781      0.577      0.439       0.46      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     71/100      1.56G      1.045      1.044      1.126         76        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.581      0.438       0.46      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     72/100      1.56G      1.043      1.037      1.123         71        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.573      0.441      0.461      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     73/100      1.56G      1.038      1.023      1.123         50        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.572      0.443      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     74/100      1.56G      1.034      1.021      1.121         99        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781       0.58      0.439      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     75/100      1.56G      1.035      1.017      1.119         31        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.583      0.436      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     76/100      1.56G      1.034      1.018      1.119         59        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.3s<0.1s\n",
      "                   all       5000      36781      0.584      0.437      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     77/100      1.56G      1.022     0.9956      1.113         95        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.584      0.438      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     78/100      1.56G      1.023      0.994      1.113         69        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 6.8it/s 45.7s<0.1s\n",
      "                   all       5000      36781      0.589      0.435      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     79/100      1.56G       1.02     0.9896      1.113         24        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.9s<0.1s\n",
      "                   all       5000      36781      0.588      0.437      0.462      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     80/100      1.56G      1.021     0.9837      1.111         34        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781      0.586      0.439      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     81/100      1.56G      1.015     0.9794       1.11         54        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.586      0.439      0.463       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     82/100      1.56G      1.011     0.9658      1.104         51        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781      0.587      0.439      0.463       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     83/100      1.56G       1.01      0.965      1.105         65        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.589      0.438      0.464       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     84/100      1.56G      1.002     0.9571      1.103         60        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.588      0.438      0.464       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     85/100      1.56G     0.9996     0.9487        1.1         49        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781      0.582      0.441      0.464       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     86/100      1.56G     0.9958     0.9415      1.097         61        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.4s<0.1s\n",
      "                   all       5000      36781      0.582      0.441      0.464       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     87/100      1.56G     0.9968      0.936      1.097         93        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.584       0.44      0.464       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     88/100      1.56G     0.9901      0.931      1.095         77        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781      0.579      0.443      0.463       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     89/100      1.56G     0.9967     0.9254      1.094         32        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.1s<0.1s\n",
      "                   all       5000      36781       0.58      0.443      0.463       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     90/100      1.56G     0.9856     0.9167       1.09         48        512: 100% ━━━━━━━━━━━━ 1479/1479 7.0it/s 3:32<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781       0.58      0.442      0.463       0.32\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     91/100      1.56G     0.9424     0.7922       1.05         44        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.0it/s 45.0s<0.1s\n",
      "                   all       5000      36781      0.583      0.441      0.463       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     92/100      1.56G     0.9273     0.7678      1.042         26        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781      0.582      0.441      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     93/100      1.56G     0.9192     0.7522      1.039         28        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.8s<0.1s\n",
      "                   all       5000      36781      0.578      0.444      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     94/100      1.56G     0.9193     0.7496      1.037         39        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.7s<0.1s\n",
      "                   all       5000      36781      0.586       0.44      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     95/100      1.56G     0.9161     0.7419      1.035         37        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.2s<0.1s\n",
      "                   all       5000      36781      0.588      0.439      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     96/100      1.56G     0.9093     0.7327      1.033         44        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:27<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.4s<0.1s\n",
      "                   all       5000      36781      0.583      0.441      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     97/100      1.56G     0.9069     0.7266       1.03         23        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.1it/s 44.1s<0.1s\n",
      "                   all       5000      36781       0.59      0.438      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     98/100      1.56G     0.9035     0.7227      1.029         10        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:29<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.3it/s 42.6s<0.1s\n",
      "                   all       5000      36781      0.592      0.436      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     99/100      1.56G     0.8995     0.7164      1.027         36        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:28<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.2s<0.1s\n",
      "                   all       5000      36781      0.592      0.436      0.463      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K    100/100      1.56G     0.8982     0.7104      1.024         23        512: 100% ━━━━━━━━━━━━ 1479/1479 7.1it/s 3:29<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.4it/s 42.5s<0.1s\n",
      "                   all       5000      36781       0.59      0.437      0.463      0.319\n",
      "\n",
      "100 epochs completed in 7.115 hours.\n",
      "Optimizer stripped from C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10\\weights\\last.pt, 22.6MB\n",
      "Optimizer stripped from C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10\\weights\\best.pt, 22.6MB\n",
      "\n",
      "Validating C:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10\\weights\\best.pt...\n",
      "Ultralytics 8.3.202  Python-3.10.18 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "Model summary (fused): 72 layers, 11,156,544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 313/313 7.6it/s 41.4s<0.1s\n",
      "                   all       5000      36781      0.631      0.484      0.522      0.366\n",
      "                person       2693      11004      0.782      0.641      0.731      0.498\n",
      "               bicycle        149        316      0.697      0.409      0.471      0.267\n",
      "                   car        535       1932      0.689      0.513      0.565      0.356\n",
      "            motorcycle        159        371      0.692      0.557       0.65      0.411\n",
      "              airplane         97        143      0.865      0.727      0.835      0.658\n",
      "                   bus        189        285      0.753      0.653      0.741      0.606\n",
      "                 train        157        190      0.813      0.774       0.86      0.675\n",
      "                 truck        250        415      0.541      0.405      0.452      0.304\n",
      "                  boat        121        430      0.532      0.331      0.366      0.193\n",
      "         traffic light        191        637      0.592      0.392      0.423      0.216\n",
      "          fire hydrant         86        101       0.77      0.743      0.796      0.623\n",
      "             stop sign         69         75      0.875      0.587      0.694      0.609\n",
      "         parking meter         37         60      0.586      0.567      0.603      0.468\n",
      "                 bench        235        413      0.554      0.294      0.313      0.207\n",
      "                  bird        125        440       0.51      0.373       0.39      0.258\n",
      "                   cat        184        202      0.804      0.793       0.84      0.656\n",
      "                   dog        177        218      0.706      0.673      0.721      0.588\n",
      "                 horse        128        273      0.739      0.648      0.712      0.531\n",
      "                 sheep         65        361       0.52      0.676      0.617      0.413\n",
      "                   cow         87        380      0.725      0.616      0.671      0.473\n",
      "              elephant         89        255      0.711      0.859      0.823      0.618\n",
      "                  bear         49         71      0.813      0.611       0.75      0.618\n",
      "                 zebra         85        268      0.769      0.834      0.886      0.654\n",
      "               giraffe        101        232      0.794      0.845      0.861      0.663\n",
      "              backpack        228        371      0.436      0.171      0.208      0.103\n",
      "              umbrella        174        413      0.644      0.545      0.577      0.377\n",
      "               handbag        292        540      0.519      0.142      0.194      0.108\n",
      "                   tie        145        254      0.778      0.372      0.441      0.284\n",
      "              suitcase        105        303      0.563      0.492      0.523      0.355\n",
      "               frisbee         84        115       0.81      0.722      0.805      0.596\n",
      "                  skis        120        241      0.608      0.324      0.382      0.194\n",
      "             snowboard         49         69       0.71       0.32      0.376      0.222\n",
      "           sports ball        169        263       0.76      0.446      0.506      0.349\n",
      "                  kite         91        336       0.57      0.565       0.54      0.357\n",
      "          baseball bat         97        146      0.627      0.438      0.448      0.257\n",
      "        baseball glove        100        148      0.702      0.473      0.529      0.308\n",
      "            skateboard        127        179      0.749      0.651      0.686      0.491\n",
      "             surfboard        149        269      0.713      0.457      0.542       0.34\n",
      "         tennis racket        167        225      0.772      0.627      0.688      0.431\n",
      "                bottle        379       1025      0.688      0.336      0.449      0.291\n",
      "            wine glass        110        343      0.665      0.335      0.413       0.27\n",
      "                   cup        390        899      0.567      0.462      0.487      0.344\n",
      "                  fork        155        215      0.638      0.368      0.428      0.293\n",
      "                 knife        181        326       0.52      0.147      0.213      0.124\n",
      "                 spoon        153        253      0.479      0.202      0.221      0.137\n",
      "                  bowl        314        626      0.612      0.476      0.507      0.366\n",
      "                banana        103        379      0.385      0.301      0.286      0.179\n",
      "                 apple         76        239      0.311      0.251      0.191      0.135\n",
      "              sandwich         98        177      0.504      0.367      0.384      0.284\n",
      "                orange         85        287      0.414      0.372      0.306      0.239\n",
      "              broccoli         71        316      0.407      0.396      0.331      0.172\n",
      "                carrot         81        371      0.421      0.275      0.281      0.169\n",
      "               hot dog         51        127       0.48      0.409      0.394       0.29\n",
      "                 pizza        153        285      0.718      0.581      0.638      0.473\n",
      "                 donut         62        338      0.534       0.45      0.422      0.326\n",
      "                  cake        124        316      0.548      0.443      0.452      0.303\n",
      "                 chair        580       1791      0.538      0.383      0.401      0.251\n",
      "                 couch        195        261      0.609      0.575      0.603       0.43\n",
      "          potted plant        172        343      0.482      0.399      0.385      0.222\n",
      "                   bed        149        163      0.522      0.589      0.555      0.385\n",
      "          dining table        501        697      0.477      0.473      0.419      0.279\n",
      "                toilet        149        179      0.706      0.721      0.771       0.62\n",
      "                    tv        207        288      0.661      0.722      0.739      0.552\n",
      "                laptop        183        231       0.75      0.688      0.735      0.597\n",
      "                 mouse         88        106      0.756      0.642      0.681      0.515\n",
      "                remote        145        283      0.575      0.258      0.331      0.197\n",
      "              keyboard        106        153      0.613      0.538      0.641      0.468\n",
      "            cell phone        214        262      0.643      0.329      0.402      0.281\n",
      "             microwave         54         55      0.684      0.527      0.676      0.523\n",
      "                  oven        115        143      0.613       0.51      0.543       0.36\n",
      "               toaster          8          9          1      0.309      0.601      0.355\n",
      "                  sink        187        225      0.639      0.444      0.497      0.315\n",
      "          refrigerator        101        126      0.634      0.627      0.651      0.495\n",
      "                  book        230       1161      0.308      0.202      0.162     0.0781\n",
      "                 clock        204        267      0.618      0.655      0.662      0.455\n",
      "                  vase        137        277      0.505      0.451      0.437      0.305\n",
      "              scissors         28         36      0.492      0.306      0.333      0.298\n",
      "            teddy bear         94        191      0.659      0.545      0.555      0.372\n",
      "            hair drier          9         11          1          0      0.116     0.0608\n",
      "            toothbrush         34         57      0.281      0.368      0.232      0.143\n",
      "Speed: 0.2ms preprocess, 2.7ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\musab\\Documents\\ULGEN_Image_Processing\\runs\\detect\\train10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data=\"./datasets/coco/coco.yaml\", epochs=100, imgsz=512, batch=8, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596bc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 1 car, 1 bus, 1 truck, 4 traffic lights, 42.8ms\n",
      "Speed: 24.7ms preprocess, 42.8ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 bus, 1 truck, 4 traffic lights, 1 handbag, 42.5ms\n",
      "Speed: 2.9ms preprocess, 42.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 4 traffic lights, 1 backpack, 1 handbag, 41.1ms\n",
      "Speed: 4.1ms preprocess, 41.1ms inference, 9.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 bus, 2 trucks, 4 traffic lights, 1 backpack, 44.0ms\n",
      "Speed: 2.9ms preprocess, 44.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 bus, 2 trucks, 5 traffic lights, 1 backpack, 41.7ms\n",
      "Speed: 2.8ms preprocess, 41.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 cars, 1 bus, 2 trucks, 4 traffic lights, 1 backpack, 42.0ms\n",
      "Speed: 5.2ms preprocess, 42.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 1 bus, 2 trucks, 4 traffic lights, 38.3ms\n",
      "Speed: 2.5ms preprocess, 38.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 1 bus, 2 trucks, 3 traffic lights, 1 handbag, 35.8ms\n",
      "Speed: 2.4ms preprocess, 35.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 2 trucks, 3 traffic lights, 35.0ms\n",
      "Speed: 5.2ms preprocess, 35.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 1 bus, 2 trucks, 3 traffic lights, 34.4ms\n",
      "Speed: 2.4ms preprocess, 34.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 1 bus, 1 truck, 3 traffic lights, 23.8ms\n",
      "Speed: 2.3ms preprocess, 23.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 3 traffic lights, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 4 traffic lights, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 2 buss, 1 truck, 3 traffic lights, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 11.9ms\n",
      "Speed: 2.2ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 2 trucks, 3 traffic lights, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 2 cars, 1 truck, 3 traffic lights, 1 suitcase, 9.5ms\n",
      "Speed: 1.8ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 1 bus, 3 traffic lights, 8.7ms\n",
      "Speed: 1.9ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 3 traffic lights, 10.5ms\n",
      "Speed: 2.2ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 3 traffic lights, 8.7ms\n",
      "Speed: 1.8ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 1 bus, 3 traffic lights, 10.7ms\n",
      "Speed: 2.7ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 1 bus, 3 traffic lights, 10.1ms\n",
      "Speed: 2.0ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 8.7ms\n",
      "Speed: 1.8ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 cars, 1 bus, 3 traffic lights, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 11.7ms\n",
      "Speed: 1.6ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 cars, 1 bus, 2 traffic lights, 1 backpack, 1 suitcase, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 1 bus, 3 traffic lights, 8.7ms\n",
      "Speed: 1.4ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 cars, 1 bus, 3 traffic lights, 17.1ms\n",
      "Speed: 1.6ms preprocess, 17.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 2 traffic lights, 17.4ms\n",
      "Speed: 2.3ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 truck, 2 traffic lights, 17.6ms\n",
      "Speed: 1.4ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 22.9ms\n",
      "Speed: 2.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 17.8ms\n",
      "Speed: 1.9ms preprocess, 17.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 17.7ms\n",
      "Speed: 2.0ms preprocess, 17.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 3 traffic lights, 18.3ms\n",
      "Speed: 2.4ms preprocess, 18.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 3 traffic lights, 17.9ms\n",
      "Speed: 1.7ms preprocess, 17.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 17.9ms\n",
      "Speed: 1.6ms preprocess, 17.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 1 handbag, 17.9ms\n",
      "Speed: 2.6ms preprocess, 17.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 1 handbag, 17.9ms\n",
      "Speed: 1.5ms preprocess, 17.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 18.0ms\n",
      "Speed: 1.6ms preprocess, 18.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 1 handbag, 1 suitcase, 17.9ms\n",
      "Speed: 1.5ms preprocess, 17.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 1 handbag, 17.9ms\n",
      "Speed: 1.4ms preprocess, 17.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 1 handbag, 17.9ms\n",
      "Speed: 1.3ms preprocess, 17.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 1 handbag, 16.1ms\n",
      "Speed: 1.4ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 16.1ms\n",
      "Speed: 1.5ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 bus, 1 truck, 3 traffic lights, 16.2ms\n",
      "Speed: 1.6ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 1 bus, 1 truck, 3 traffic lights, 1 suitcase, 16.2ms\n",
      "Speed: 1.7ms preprocess, 16.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 bus, 1 truck, 3 traffic lights, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bus, 1 truck, 3 traffic lights, 16.1ms\n",
      "Speed: 2.0ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bus, 3 traffic lights, 16.2ms\n",
      "Speed: 1.8ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 bus, 3 traffic lights, 1 suitcase, 16.2ms\n",
      "Speed: 1.9ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 bus, 3 traffic lights, 1 suitcase, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bus, 3 traffic lights, 1 suitcase, 16.1ms\n",
      "Speed: 2.1ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bus, 3 traffic lights, 1 suitcase, 17.0ms\n",
      "Speed: 2.3ms preprocess, 17.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bus, 3 traffic lights, 1 suitcase, 16.2ms\n",
      "Speed: 1.7ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bus, 3 traffic lights, 1 suitcase, 16.3ms\n",
      "Speed: 1.7ms preprocess, 16.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bus, 2 traffic lights, 1 suitcase, 15.3ms\n",
      "Speed: 1.7ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 suitcase, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 suitcase, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 handbag, 1 suitcase, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 suitcase, 14.9ms\n",
      "Speed: 2.4ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 traffic lights, 21.6ms\n",
      "Speed: 1.6ms preprocess, 21.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 traffic lights, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4 traffic lights, 1 handbag, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 traffic lights, 1 handbag, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 traffic lights, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 traffic lights, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 3 traffic lights, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 backpack, 21.2ms\n",
      "Speed: 2.4ms preprocess, 21.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 backpack, 15.2ms\n",
      "Speed: 2.8ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.8ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.8ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 3 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 2 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 3 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 3 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 suitcase, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 2 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 2 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 traffic lights, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 traffic lights, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 4 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 4 traffic lights, 1 backpack, 14.7ms\n",
      "Speed: 1.5ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 traffic lights, 14.7ms\n",
      "Speed: 2.0ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 traffic lights, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 traffic lights, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 2 traffic lights, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 3 traffic lights, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 backpack, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 backpack, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 traffic lights, 1 backpack, 17.6ms\n",
      "Speed: 1.8ms preprocess, 17.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 4 traffic lights, 1 backpack, 15.6ms\n",
      "Speed: 2.7ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 2.7ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 traffic lights, 2 backpacks, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 2 backpacks, 15.4ms\n",
      "Speed: 2.6ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 2 backpacks, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 3 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 2 backpacks, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 3 traffic lights, 2 backpacks, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 traffic lights, 3 backpacks, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 traffic lights, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 2 traffic lights, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 3 traffic lights, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 backpack, 1 handbag, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 2 traffic lights, 2 backpacks, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 1 backpack, 17.6ms\n",
      "Speed: 4.0ms preprocess, 17.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 traffic lights, 1 backpack, 15.9ms\n",
      "Speed: 2.7ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 15.8ms\n",
      "Speed: 3.0ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 2 backpacks, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 2 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 2 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 car, 3 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 2.7ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 car, 4 traffic lights, 2 backpacks, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 2 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 traffic lights, 1 backpack, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 backpack, 1 dining table, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 4 traffic lights, 1 backpack, 1 dining table, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 traffic lights, 1 backpack, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 1 backpack, 1 handbag, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 14.9ms\n",
      "Speed: 2.8ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 2 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 3 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 car, 2 traffic lights, 1 backpack, 1 handbag, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 4 traffic lights, 2 backpacks, 1 handbag, 2 suitcases, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 4 traffic lights, 1 backpack, 2 suitcases, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 car, 4 traffic lights, 1 backpack, 2 suitcases, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 4 traffic lights, 3 backpacks, 1 suitcase, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 3 traffic lights, 1 backpack, 2 suitcases, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 car, 3 traffic lights, 3 backpacks, 1 suitcase, 14.7ms\n",
      "Speed: 2.5ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 3 traffic lights, 2 backpacks, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 3 backpacks, 2 suitcases, 14.7ms\n",
      "Speed: 2.3ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 traffic lights, 2 backpacks, 1 suitcase, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 traffic lights, 3 backpacks, 1 suitcase, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 traffic lights, 2 backpacks, 2 suitcases, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 traffic lights, 1 backpack, 1 suitcase, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 3 traffic lights, 1 backpack, 1 handbag, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 car, 2 traffic lights, 1 backpack, 1 suitcase, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 backpack, 1 suitcase, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 1 backpack, 1 suitcase, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 car, 2 traffic lights, 2 backpacks, 1 handbag, 1 suitcase, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 car, 2 traffic lights, 2 backpacks, 1 handbag, 15.2ms\n",
      "Speed: 2.9ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 14.7ms\n",
      "Speed: 2.1ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 14.7ms\n",
      "Speed: 1.8ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 traffic lights, 1 backpack, 2 suitcases, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 2.3ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 5 traffic lights, 1 backpack, 3 suitcases, 14.5ms\n",
      "Speed: 2.0ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 5 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 4 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 2.3ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 5 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 1.5ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 3 suitcases, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 4 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 3 suitcases, 14.5ms\n",
      "Speed: 1.6ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 3 suitcases, 14.6ms\n",
      "Speed: 1.5ms preprocess, 14.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 4 traffic lights, 3 suitcases, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 4 traffic lights, 3 suitcases, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 3 suitcases, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 2.5ms preprocess, 14.6ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.6ms\n",
      "Speed: 2.0ms preprocess, 14.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 3 traffic lights, 1 backpack, 3 suitcases, 14.5ms\n",
      "Speed: 1.6ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 3 traffic lights, 2 backpacks, 3 suitcases, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 4 traffic lights, 2 suitcases, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 2 suitcases, 1 potted plant, 20.6ms\n",
      "Speed: 3.2ms preprocess, 20.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 2 suitcases, 1 potted plant, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 traffic lights, 1 backpack, 2 suitcases, 1 potted plant, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 traffic lights, 2 backpacks, 3 suitcases, 15.3ms\n",
      "Speed: 1.7ms preprocess, 15.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 3 backpacks, 3 suitcases, 1 potted plant, 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 traffic lights, 2 backpacks, 2 suitcases, 15.4ms\n",
      "Speed: 1.7ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 2 suitcases, 15.4ms\n",
      "Speed: 1.8ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 backpack, 2 suitcases, 15.5ms\n",
      "Speed: 1.6ms preprocess, 15.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 traffic lights, 2 backpacks, 2 suitcases, 15.5ms\n",
      "Speed: 1.8ms preprocess, 15.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 traffic lights, 2 backpacks, 1 handbag, 2 suitcases, 15.5ms\n",
      "Speed: 1.5ms preprocess, 15.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 2 backpacks, 1 handbag, 2 suitcases, 15.5ms\n",
      "Speed: 1.8ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 4 traffic lights, 1 backpack, 3 suitcases, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 4 traffic lights, 2 backpacks, 3 suitcases, 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 2 backpacks, 3 suitcases, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 3 suitcases, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 cars, 3 traffic lights, 2 backpacks, 4 suitcases, 15.2ms\n",
      "Speed: 2.8ms preprocess, 15.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 car, 3 traffic lights, 1 backpack, 3 suitcases, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 traffic lights, 1 handbag, 3 suitcases, 15.1ms\n",
      "Speed: 2.6ms preprocess, 15.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 traffic lights, 1 handbag, 3 suitcases, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 3 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 traffic lights, 1 backpack, 1 handbag, 3 suitcases, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 4 traffic lights, 1 handbag, 3 suitcases, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 5 traffic lights, 2 backpacks, 1 handbag, 1 suitcase, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 backpack, 1 handbag, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 backpack, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 handbag, 3 suitcases, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 1 backpack, 1 handbag, 2 suitcases, 14.9ms\n",
      "Speed: 1.4ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 4 traffic lights, 1 handbag, 2 suitcases, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 traffic lights, 1 backpack, 1 handbag, 15.0ms\n",
      "Speed: 2.3ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 traffic lights, 1 backpack, 1 suitcase, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 traffic lights, 1 handbag, 1 suitcase, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 1.9ms preprocess, 15.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 4 traffic lights, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 1 handbag, 15.8ms\n",
      "Speed: 1.8ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 1 handbag, 15.7ms\n",
      "Speed: 1.7ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 1 handbag, 15.7ms\n",
      "Speed: 1.8ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 1 handbag, 15.7ms\n",
      "Speed: 1.7ms preprocess, 15.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 handbag, 15.8ms\n",
      "Speed: 1.7ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 traffic lights, 1 handbag, 15.7ms\n",
      "Speed: 1.9ms preprocess, 15.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 traffic lights, 1 handbag, 15.7ms\n",
      "Speed: 1.6ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 handbag, 15.8ms\n",
      "Speed: 1.6ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 traffic lights, 1 handbag, 16.0ms\n",
      "Speed: 1.6ms preprocess, 16.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 1 handbag, 16.3ms\n",
      "Speed: 1.9ms preprocess, 16.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 16.0ms\n",
      "Speed: 1.7ms preprocess, 16.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 traffic lights, 1 handbag, 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 3 traffic lights, 1 handbag, 16.8ms\n",
      "Speed: 1.6ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 3 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 3 traffic lights, 1 handbag, 15.5ms\n",
      "Speed: 2.3ms preprocess, 15.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 3 traffic lights, 1 handbag, 15.4ms\n",
      "Speed: 2.3ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 3 traffic lights, 2 handbags, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 4 traffic lights, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 1 potted plant, 15.2ms\n",
      "Speed: 2.1ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 2 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 2.0ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 2 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 2.0ms preprocess, 15.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 cars, 2 traffic lights, 1 handbag, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3 cars, 3 traffic lights, 1 handbag, 15.5ms\n",
      "Speed: 1.6ms preprocess, 15.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 cars, 2 traffic lights, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 2 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 cars, 2 traffic lights, 1 suitcase, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 2.4ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 2 traffic lights, 1 handbag, 20.0ms\n",
      "Speed: 1.9ms preprocess, 20.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 2 traffic lights, 2 handbags, 15.4ms\n",
      "Speed: 2.8ms preprocess, 15.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 2 traffic lights, 2 handbags, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 3 cars, 2 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 cars, 2 traffic lights, 2 handbags, 1 clock, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 3 traffic lights, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 2 traffic lights, 1 handbag, 1 clock, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 cars, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 cars, 3 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 cars, 3 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 car, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 2 handbags, 1 suitcase, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 cars, 3 traffic lights, 2 handbags, 1 suitcase, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 2 handbags, 1 clock, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 traffic lights, 2 handbags, 1 clock, 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 traffic lights, 1 handbag, 1 clock, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 traffic lights, 1 handbag, 1 clock, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 traffic lights, 1 handbag, 1 clock, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 1 clock, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 1 clock, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 clock, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 clock, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 clock, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 2 traffic lights, 1 clock, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 traffic lights, 1 clock, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 2 traffic lights, 1 clock, 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 2 traffic lights, 1 clock, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 2 traffic lights, 1 clock, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 2 traffic lights, 1 clock, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 2 traffic lights, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 traffic lights, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 2 traffic lights, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 2 traffic lights, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 4 traffic lights, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 3 traffic lights, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cars, 1 boat, 5 traffic lights, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 boat, 4 traffic lights, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 boat, 4 traffic lights, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 boat, 4 traffic lights, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 4 traffic lights, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 3 traffic lights, 1 clock, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 traffic lights, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 traffic lights, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 2 traffic lights, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 2 traffic lights, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cars, 2 traffic lights, 1 clock, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 cars, 3 traffic lights, 1 clock, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 2 traffic lights, 1 clock, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 traffic lights, 1 clock, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 2 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 2 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 3 traffic lights, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 traffic lights, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 backpack, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 3 traffic lights, 15.0ms\n",
      "Speed: 2.3ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 traffic lights, 16.1ms\n",
      "Speed: 2.5ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 traffic lights, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 2 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 traffic lights, 1 handbag, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 2 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 car, 3 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 4 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 1 clock, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 1 handbag, 1 clock, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 3 traffic lights, 1 handbag, 1 clock, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 15.1ms\n",
      "Speed: 2.3ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 3 traffic lights, 1 handbag, 1 clock, 15.4ms\n",
      "Speed: 2.8ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 2 traffic lights, 1 handbag, 1 donut, 1 clock, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 15.3ms\n",
      "Speed: 1.8ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 1 handbag, 1 clock, 15.9ms\n",
      "Speed: 1.9ms preprocess, 15.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 clock, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 handbag, 1 clock, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 handbag, 1 clock, 33.4ms\n",
      "Speed: 3.2ms preprocess, 33.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 1 clock, 22.6ms\n",
      "Speed: 2.0ms preprocess, 22.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 handbag, 1 clock, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 handbag, 1 clock, 17.0ms\n",
      "Speed: 2.1ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 clock, 17.1ms\n",
      "Speed: 2.4ms preprocess, 17.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 clock, 17.0ms\n",
      "Speed: 1.9ms preprocess, 17.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 handbag, 1 clock, 17.2ms\n",
      "Speed: 1.9ms preprocess, 17.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 1 handbag, 1 clock, 17.1ms\n",
      "Speed: 1.8ms preprocess, 17.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 1 clock, 17.1ms\n",
      "Speed: 1.7ms preprocess, 17.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 handbag, 1 clock, 17.2ms\n",
      "Speed: 2.0ms preprocess, 17.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 handbag, 1 clock, 17.3ms\n",
      "Speed: 2.4ms preprocess, 17.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 1 handbag, 1 clock, 17.1ms\n",
      "Speed: 1.8ms preprocess, 17.1ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 1 clock, 17.7ms\n",
      "Speed: 2.5ms preprocess, 17.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 handbag, 1 clock, 17.3ms\n",
      "Speed: 1.8ms preprocess, 17.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 clock, 17.5ms\n",
      "Speed: 1.9ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 1 donut, 1 clock, 17.4ms\n",
      "Speed: 2.4ms preprocess, 17.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 1 donut, 1 clock, 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 1 donut, 1 clock, 17.3ms\n",
      "Speed: 1.6ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 traffic light, 1 donut, 17.0ms\n",
      "Speed: 1.7ms preprocess, 17.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 17.0ms\n",
      "Speed: 3.7ms preprocess, 17.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 17.5ms\n",
      "Speed: 1.9ms preprocess, 17.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 16.9ms\n",
      "Speed: 1.8ms preprocess, 16.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 donut, 17.0ms\n",
      "Speed: 2.1ms preprocess, 17.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 clock, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 truck, 1 traffic light, 16.8ms\n",
      "Speed: 1.9ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 16.8ms\n",
      "Speed: 2.1ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 truck, 1 traffic light, 16.9ms\n",
      "Speed: 1.7ms preprocess, 16.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 1 traffic light, 16.9ms\n",
      "Speed: 1.6ms preprocess, 16.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 traffic lights, 17.1ms\n",
      "Speed: 1.9ms preprocess, 17.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 traffic lights, 1 potted plant, 16.8ms\n",
      "Speed: 1.7ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 truck, 2 traffic lights, 1 potted plant, 16.9ms\n",
      "Speed: 1.9ms preprocess, 16.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 truck, 2 traffic lights, 1 potted plant, 16.9ms\n",
      "Speed: 2.0ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 truck, 1 traffic light, 1 clock, 16.9ms\n",
      "Speed: 2.5ms preprocess, 16.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 potted plant, 1 clock, 16.9ms\n",
      "Speed: 2.0ms preprocess, 16.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 truck, 2 traffic lights, 1 chair, 1 clock, 17.3ms\n",
      "Speed: 1.7ms preprocess, 17.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 chair, 1 clock, 16.1ms\n",
      "Speed: 1.9ms preprocess, 16.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 chair, 1 clock, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 2 traffic lights, 1 chair, 16.1ms\n",
      "Speed: 2.1ms preprocess, 16.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 1 chair, 15.9ms\n",
      "Speed: 2.4ms preprocess, 15.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 2 chairs, 16.1ms\n",
      "Speed: 2.0ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 truck, 2 traffic lights, 1 chair, 15.9ms\n",
      "Speed: 1.7ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 traffic lights, 1 handbag, 1 donut, 1 chair, 16.1ms\n",
      "Speed: 1.5ms preprocess, 16.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 traffic lights, 1 handbag, 1 donut, 1 chair, 15.9ms\n",
      "Speed: 1.6ms preprocess, 15.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 2 handbags, 1 donut, 2 chairs, 1 potted plant, 16.1ms\n",
      "Speed: 1.9ms preprocess, 16.1ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 2 handbags, 1 chair, 17.4ms\n",
      "Speed: 3.4ms preprocess, 17.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 3 handbags, 1 chair, 16.0ms\n",
      "Speed: 2.7ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 traffic lights, 1 handbag, 1 chair, 15.9ms\n",
      "Speed: 1.6ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 1 handbag, 1 chair, 16.0ms\n",
      "Speed: 1.6ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 traffic lights, 2 handbags, 1 donut, 1 chair, 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 chair, 16.0ms\n",
      "Speed: 1.6ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 umbrella, 1 donut, 1 chair, 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 1 umbrella, 1 chair, 16.0ms\n",
      "Speed: 1.6ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 umbrella, 1 donut, 1 chair, 15.8ms\n",
      "Speed: 1.8ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 umbrella, 1 chair, 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 umbrella, 1 chair, 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 1 chair, 16.0ms\n",
      "Speed: 1.5ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 chair, 15.3ms\n",
      "Speed: 2.1ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 chair, 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 umbrella, 1 chair, 15.3ms\n",
      "Speed: 2.0ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 umbrella, 1 chair, 15.3ms\n",
      "Speed: 1.7ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 umbrella, 1 handbag, 1 chair, 15.4ms\n",
      "Speed: 1.7ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 3 handbags, 1 chair, 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 3 handbags, 1 chair, 15.3ms\n",
      "Speed: 1.6ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 4 handbags, 1 chair, 15.4ms\n",
      "Speed: 2.4ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 handbag, 1 chair, 15.7ms\n",
      "Speed: 2.3ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 2 handbags, 1 chair, 15.3ms\n",
      "Speed: 1.6ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 handbag, 1 donut, 1 chair, 15.4ms\n",
      "Speed: 1.5ms preprocess, 15.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 handbag, 1 chair, 15.3ms\n",
      "Speed: 1.5ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 chair, 15.4ms\n",
      "Speed: 1.7ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 handbag, 1 chair, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 chair, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 chair, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 handbag, 1 chair, 1 potted plant, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 chair, 1 potted plant, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 chair, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 chair, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 chair, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 chair, 15.1ms\n",
      "Speed: 3.3ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 1 traffic light, 1 handbag, 1 chair, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 1 traffic light, 1 handbag, 1 chair, 1 potted plant, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 1 chair, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 truck, 1 traffic light, 1 handbag, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 traffic light, 1 chair, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 1 chair, 20.1ms\n",
      "Speed: 2.0ms preprocess, 20.1ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 cars, 1 traffic light, 1 handbag, 1 chair, 18.8ms\n",
      "Speed: 2.1ms preprocess, 18.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 donut, 1 chair, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 1 chair, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 traffic light, 1 handbag, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 car, 1 traffic light, 2 handbags, 1 chair, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 car, 1 traffic light, 2 handbags, 1 chair, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 traffic light, 2 handbags, 1 donut, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 1 traffic light, 2 handbags, 14.7ms\n",
      "Speed: 1.8ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 car, 1 traffic light, 1 handbag, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 traffic light, 1 handbag, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 2 handbags, 16.8ms\n",
      "Speed: 2.4ms preprocess, 16.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 traffic lights, 2 handbags, 14.8ms\n",
      "Speed: 2.4ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 traffic lights, 2 handbags, 18.5ms\n",
      "Speed: 2.7ms preprocess, 18.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 1 handbag, 16.4ms\n",
      "Speed: 3.3ms preprocess, 16.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 14.8ms\n",
      "Speed: 2.5ms preprocess, 14.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 14.9ms\n",
      "Speed: 2.3ms preprocess, 14.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 handbag, 1 clock, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 2 handbags, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 4 handbags, 1 clock, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 2 handbags, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 2 handbags, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 4 handbags, 1 clock, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 traffic light, 3 handbags, 20.0ms\n",
      "Speed: 2.6ms preprocess, 20.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 3 handbags, 1 clock, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 handbag, 1 potted plant, 1 clock, 14.9ms\n",
      "Speed: 1.9ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 1 clock, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 1 handbag, 1 clock, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 1 clock, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 1 clock, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 1 clock, 14.7ms\n",
      "Speed: 1.8ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 1 clock, 14.7ms\n",
      "Speed: 2.1ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 1 clock, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 handbags, 1 clock, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1 clock, 14.7ms\n",
      "Speed: 1.5ms preprocess, 14.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 1 clock, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 1 clock, 15.1ms\n",
      "Speed: 1.5ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 1 handbag, 1 potted plant, 14.7ms\n",
      "Speed: 1.5ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1 potted plant, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 14.7ms\n",
      "Speed: 2.1ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 3 handbags, 1 potted plant, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 2 handbags, 1 potted plant, 15.3ms\n",
      "Speed: 1.8ms preprocess, 15.3ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 1 potted plant, 15.8ms\n",
      "Speed: 1.9ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 2 handbags, 15.7ms\n",
      "Speed: 1.6ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 2 handbags, 15.7ms\n",
      "Speed: 2.4ms preprocess, 15.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 2 handbags, 15.6ms\n",
      "Speed: 1.6ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 1 handbag, 15.7ms\n",
      "Speed: 1.8ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 3 handbags, 1 potted plant, 15.8ms\n",
      "Speed: 1.8ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 1 handbag, 1 potted plant, 15.7ms\n",
      "Speed: 1.6ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 2 handbags, 2 potted plants, 15.8ms\n",
      "Speed: 2.4ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 3 handbags, 2 potted plants, 15.7ms\n",
      "Speed: 1.7ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 3 handbags, 2 potted plants, 15.9ms\n",
      "Speed: 2.1ms preprocess, 15.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 2 handbags, 1 potted plant, 15.7ms\n",
      "Speed: 1.5ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 2 handbags, 1 potted plant, 15.7ms\n",
      "Speed: 1.6ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 1 potted plant, 15.7ms\n",
      "Speed: 1.7ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 2 potted plants, 15.8ms\n",
      "Speed: 1.5ms preprocess, 15.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 1 potted plant, 25.8ms\n",
      "Speed: 2.6ms preprocess, 25.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 1 handbag, 15.8ms\n",
      "Speed: 2.2ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 potted plant, 15.8ms\n",
      "Speed: 1.8ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 2 clocks, 16.2ms\n",
      "Speed: 1.6ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 2 clocks, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 2 handbags, 1 potted plant, 1 clock, 16.3ms\n",
      "Speed: 1.7ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 2 handbags, 16.2ms\n",
      "Speed: 1.9ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 umbrella, 3 handbags, 16.6ms\n",
      "Speed: 1.7ms preprocess, 16.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 2 handbags, 16.1ms\n",
      "Speed: 1.8ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 1 umbrella, 2 handbags, 16.3ms\n",
      "Speed: 2.2ms preprocess, 16.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 1 handbag, 16.2ms\n",
      "Speed: 1.8ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 1 handbag, 1 potted plant, 1 clock, 16.3ms\n",
      "Speed: 1.7ms preprocess, 16.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 umbrellas, 1 handbag, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 1 handbag, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 umbrella, 1 handbag, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 umbrella, 2 handbags, 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 umbrella, 3 handbags, 1 potted plant, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 2 handbags, 16.0ms\n",
      "Speed: 1.7ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 1 umbrella, 3 handbags, 16.0ms\n",
      "Speed: 1.9ms preprocess, 16.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 umbrella, 1 handbag, 1 clock, 16.0ms\n",
      "Speed: 1.7ms preprocess, 16.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 16.6ms\n",
      "Speed: 1.8ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 umbrella, 16.0ms\n",
      "Speed: 1.7ms preprocess, 16.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 1 umbrella, 1 handbag, 15.9ms\n",
      "Speed: 1.7ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 16.0ms\n",
      "Speed: 2.4ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 2 umbrellas, 16.6ms\n",
      "Speed: 2.6ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 1 handbag, 15.2ms\n",
      "Speed: 1.9ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 1 handbag, 15.3ms\n",
      "Speed: 1.5ms preprocess, 15.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 1 handbag, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 umbrellas, 1 clock, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 1 backpack, 1 umbrella, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 backpack, 1 umbrella, 1 handbag, 18.4ms\n",
      "Speed: 2.5ms preprocess, 18.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 umbrella, 15.7ms\n",
      "Speed: 3.4ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 1 backpack, 2 umbrellas, 1 clock, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 handbag, 15.5ms\n",
      "Speed: 1.6ms preprocess, 15.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 backpack, 1 umbrella, 1 handbag, 15.2ms\n",
      "Speed: 2.0ms preprocess, 15.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 handbag, 15.2ms\n",
      "Speed: 1.5ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 2 handbags, 15.6ms\n",
      "Speed: 1.8ms preprocess, 15.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 backpack, 1 umbrella, 1 handbag, 1 clock, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 backpack, 1 umbrella, 1 handbag, 1 clock, 16.8ms\n",
      "Speed: 2.1ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 backpack, 1 umbrella, 1 handbag, 1 clock, 15.9ms\n",
      "Speed: 2.2ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 umbrella, 1 handbag, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 15.2ms\n",
      "Speed: 2.1ms preprocess, 15.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 1 handbag, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 traffic light, 2 umbrellas, 2 handbags, 15.4ms\n",
      "Speed: 1.8ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 15.3ms\n",
      "Speed: 1.6ms preprocess, 15.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 15.2ms\n",
      "Speed: 1.9ms preprocess, 15.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 3 handbags, 15.4ms\n",
      "Speed: 1.7ms preprocess, 15.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 2.5ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 umbrellas, 1 handbag, 15.5ms\n",
      "Speed: 1.6ms preprocess, 15.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 umbrellas, 1 handbag, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 traffic light, 2 umbrellas, 1 handbag, 15.5ms\n",
      "Speed: 1.8ms preprocess, 15.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 1 handbag, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 1 umbrella, 2 handbags, 1 clock, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 1 handbag, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 2 handbags, 15.3ms\n",
      "Speed: 1.5ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 2 handbags, 15.5ms\n",
      "Speed: 1.6ms preprocess, 15.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 umbrellas, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 1 handbag, 15.9ms\n",
      "Speed: 29.8ms preprocess, 15.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 15.8ms\n",
      "Speed: 1.9ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 1 handbag, 15.7ms\n",
      "Speed: 1.6ms preprocess, 15.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 1 handbag, 15.8ms\n",
      "Speed: 1.6ms preprocess, 15.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 1 handbag, 17.7ms\n",
      "Speed: 1.9ms preprocess, 17.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 16.1ms\n",
      "Speed: 2.3ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 1 handbag, 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 umbrellas, 1 handbag, 16.3ms\n",
      "Speed: 2.7ms preprocess, 16.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 umbrellas, 1 handbag, 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 umbrellas, 1 handbag, 16.2ms\n",
      "Speed: 1.6ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 1 handbag, 16.3ms\n",
      "Speed: 1.7ms preprocess, 16.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 1 handbag, 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 1 handbag, 16.2ms\n",
      "Speed: 1.6ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 1 handbag, 15.1ms\n",
      "Speed: 1.5ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 1 handbag, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 umbrellas, 1 handbag, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 umbrellas, 1 handbag, 15.4ms\n",
      "Speed: 2.0ms preprocess, 15.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 umbrellas, 2 handbags, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.9ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 umbrella, 3 handbags, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 umbrellas, 4 handbags, 15.4ms\n",
      "Speed: 2.3ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 umbrellas, 3 handbags, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 2 umbrellas, 3 handbags, 15.3ms\n",
      "Speed: 2.1ms preprocess, 15.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 car, 3 umbrellas, 3 handbags, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.8ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 umbrellas, 2 handbags, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bicycle, 2 umbrellas, 2 handbags, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bicycle, 1 car, 2 umbrellas, 3 handbags, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 2 umbrellas, 1 handbag, 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 car, 2 umbrellas, 2 handbags, 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 car, 2 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 2.1ms preprocess, 15.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 car, 2 umbrellas, 1 handbag, 15.4ms\n",
      "Speed: 2.6ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 1 car, 2 umbrellas, 15.1ms\n",
      "Speed: 1.7ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 car, 2 umbrellas, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 1 car, 3 umbrellas, 1 handbag, 15.1ms\n",
      "Speed: 2.2ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 car, 1 traffic light, 3 umbrellas, 15.4ms\n",
      "Speed: 2.2ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 1 car, 1 traffic light, 3 umbrellas, 15.2ms\n",
      "Speed: 1.9ms preprocess, 15.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bicycle, 1 car, 3 umbrellas, 1 handbag, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 1 car, 2 umbrellas, 2 handbags, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 2 umbrellas, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 2 umbrellas, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 car, 1 traffic light, 2 umbrellas, 1 handbag, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 traffic light, 2 umbrellas, 2 handbags, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 traffic light, 2 umbrellas, 14.7ms\n",
      "Speed: 1.6ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 umbrellas, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 bicycle, 2 umbrellas, 14.8ms\n",
      "Speed: 2.2ms preprocess, 14.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 2 umbrellas, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 2 umbrellas, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 2 umbrellas, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 3 umbrellas, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 3 umbrellas, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 3 umbrellas, 14.9ms\n",
      "Speed: 1.8ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 1.5ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 1.7ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 1 car, 3 umbrellas, 14.9ms\n",
      "Speed: 2.4ms preprocess, 14.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 2 umbrellas, 14.7ms\n",
      "Speed: 2.3ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 2 umbrellas, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 2 umbrellas, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 1 traffic light, 3 umbrellas, 14.6ms\n",
      "Speed: 2.2ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 1 traffic light, 3 umbrellas, 14.6ms\n",
      "Speed: 1.5ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 1 traffic light, 4 umbrellas, 14.7ms\n",
      "Speed: 2.0ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 1 traffic light, 3 umbrellas, 14.7ms\n",
      "Speed: 1.8ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 1 traffic light, 2 umbrellas, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 2 umbrellas, 14.6ms\n",
      "Speed: 2.0ms preprocess, 14.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 2 umbrellas, 14.5ms\n",
      "Speed: 1.9ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 2 umbrellas, 14.6ms\n",
      "Speed: 2.7ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 2.3ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 1 traffic light, 3 umbrellas, 14.6ms\n",
      "Speed: 2.2ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 1.5ms preprocess, 14.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.5ms\n",
      "Speed: 1.7ms preprocess, 14.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.7ms\n",
      "Speed: 2.2ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.7ms\n",
      "Speed: 1.7ms preprocess, 14.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 14.5ms\n",
      "Speed: 1.9ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 14.6ms\n",
      "Speed: 2.5ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 1.7ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 2 umbrellas, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 bicycles, 1 car, 3 umbrellas, 16.5ms\n",
      "Speed: 3.5ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 2.0ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 2.3ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 bicycles, 1 car, 3 umbrellas, 14.6ms\n",
      "Speed: 2.0ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 2 bicycles, 3 umbrellas, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 3 bicycles, 3 umbrellas, 14.5ms\n",
      "Speed: 1.7ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 3 bicycles, 3 umbrellas, 14.7ms\n",
      "Speed: 2.7ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 bicycles, 3 umbrellas, 15.4ms\n",
      "Speed: 1.6ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 bicycles, 3 umbrellas, 15.3ms\n",
      "Speed: 1.8ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 bicycles, 3 umbrellas, 15.4ms\n",
      "Speed: 1.7ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 4 umbrellas, 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 5 umbrellas, 15.3ms\n",
      "Speed: 2.1ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 5 umbrellas, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 4 umbrellas, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 bicycles, 4 umbrellas, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 bicycles, 4 umbrellas, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 3 umbrellas, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 bicycles, 2 umbrellas, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 bicycles, 2 umbrellas, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 bicycles, 3 umbrellas, 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 bicycles, 1 car, 3 umbrellas, 16.2ms\n",
      "Speed: 1.7ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 1 car, 2 umbrellas, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 bicycles, 1 car, 3 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 1 car, 4 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 2 cars, 4 umbrellas, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 bicycles, 2 cars, 3 umbrellas, 15.6ms\n",
      "Speed: 2.5ms preprocess, 15.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 2 cars, 2 umbrellas, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 bicycles, 2 cars, 3 umbrellas, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 2 cars, 4 umbrellas, 15.2ms\n",
      "Speed: 2.7ms preprocess, 15.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 2 cars, 3 umbrellas, 19.0ms\n",
      "Speed: 2.4ms preprocess, 19.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 1 car, 2 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 1 car, 3 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 bicycles, 1 car, 2 umbrellas, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 bicycles, 2 cars, 2 umbrellas, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 3 cars, 2 umbrellas, 15.2ms\n",
      "Speed: 2.1ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 bicycles, 2 cars, 2 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 bicycles, 2 cars, 2 umbrellas, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 bicycles, 2 cars, 3 umbrellas, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 bicycles, 1 car, 3 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 1 car, 3 umbrellas, 15.0ms\n",
      "Speed: 1.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 bicycles, 2 cars, 4 umbrellas, 14.9ms\n",
      "Speed: 2.4ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 bicycles, 1 car, 4 umbrellas, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 bicycles, 1 car, 3 umbrellas, 14.9ms\n",
      "Speed: 2.4ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 3 bicycles, 1 car, 3 umbrellas, 15.0ms\n",
      "Speed: 1.8ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 bicycles, 1 car, 3 umbrellas, 1 chair, 15.0ms\n",
      "Speed: 2.7ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 bicycles, 4 umbrellas, 1 chair, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 4 umbrellas, 15.0ms\n",
      "Speed: 1.6ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 2 umbrellas, 1 chair, 15.3ms\n",
      "Speed: 1.7ms preprocess, 15.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 19.2ms\n",
      "Speed: 2.5ms preprocess, 19.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 16.0ms\n",
      "Speed: 2.9ms preprocess, 16.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 bicycle, 2 umbrellas, 14.9ms\n",
      "Speed: 2.3ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 bicycle, 2 umbrellas, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 2 umbrellas, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 2 umbrellas, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 3 umbrellas, 15.2ms\n",
      "Speed: 2.5ms preprocess, 15.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 2.2ms preprocess, 14.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.9ms\n",
      "Speed: 1.7ms preprocess, 14.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 2 umbrellas, 14.8ms\n",
      "Speed: 1.9ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bicycle, 2 umbrellas, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bicycle, 3 umbrellas, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bicycle, 2 umbrellas, 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "model = YOLO(\"runs/detect/train10/weights/best.pt\")\n",
    "cap = cv2.VideoCapture(\"test.mp4\")\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: \n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "\n",
    "    res = model(frame, imgsz=640, conf=0.4)[0]\n",
    "    for b in res.boxes:\n",
    "        x1,y1,x2,y2 = map(int, b.xyxy[0])\n",
    "        cls = int(b.cls[0]); conf = float(b.conf[0])\n",
    "        cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "        cv2.putText(frame, f\"{model.names[cls]} {conf*100:.1f}%\", (x1,y1-6),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "    cv2.imshow(\"Detections\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ulgen-yolo-object-det",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
